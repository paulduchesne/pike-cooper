{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intial injection of film data.**\n",
    "\n",
    "Create the ontology scaffolding for Wikibase instance, and inject pike-cooper film data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib, pandas, numpy, pydash, time, datetime, json\n",
    "from wikibase_api import Wikibase\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\"\"\"\n",
    "Define functions to create properties and items,\n",
    "also create universal items and the properties which will structure this instance.\n",
    "\"\"\"\n",
    "\n",
    "def add_property(label, description, flavour, data):\n",
    "    \n",
    "    content = {\"labels\":{\"en\":{\"language\":\"en\",\"value\":label}},\n",
    "            \"descriptions\":{\"en\":{\"language\":\"en\",\"value\":description}},\n",
    "            \"datatype\":flavour}\n",
    "    r = wb.entity.add(\"property\", content)\n",
    "    data.append(r)\n",
    "    time.sleep(1)    \n",
    "    return(r)    \n",
    "    \n",
    "def add_item(label, description, data):\n",
    "    content = {\"labels\":{\"en\":{\"language\":\"en\",\"value\":label}},\n",
    "            \"descriptions\":{\"en\":{\"language\":\"en\",\"value\":description}}}\n",
    "    r = wb.entity.add(\"item\", content)\n",
    "    data.append(r)    \n",
    "    time.sleep(1)    \n",
    "    return(r)\n",
    "\n",
    "config_path = pathlib.Path.cwd().resolve().parents[0] / 'data' / '1_wikibase_instance' / 'config.json'\n",
    "wb = Wikibase(config_path=config_path)\n",
    "\n",
    "scaffold = list()\n",
    "\n",
    "add_property('instance of', 'the class of which this subject is a particular example', 'wikibase-item', scaffold)\n",
    "add_property('title', 'title of work', 'string', scaffold)\n",
    "add_property('year', 'year of work', 'time', scaffold)\n",
    "add_property('country of origin', 'year of work', 'wikibase-item', scaffold)\n",
    "add_property('director', 'director of work', 'wikibase-item', scaffold)\n",
    "add_property('pike-cooper id', 'pike-cooper identifier', 'string', scaffold)\n",
    "add_item('cinematographic work', 'This entity forms the node that relates all variants and manifestations of a moving image work to a common creation.', scaffold)\n",
    "add_item('Australia', 'Country in Oceania', scaffold)\n",
    "\n",
    "prebuilt = {pydash.get(x, 'entity.labels.en.value'):pydash.get(x, 'entity.id') for x in scaffold}\n",
    "save_path = pathlib.Path.cwd().resolve().parents[0] / 'data' / '1_wikibase_instance' / 'prebuilt.json'\n",
    "with open(save_path, 'w') as intial_struct:\n",
    "    json.dump(prebuilt, intial_struct)\n",
    "    \n",
    "print(prebuilt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Inject pike-cooper film title data.\n",
    "\"\"\"\n",
    "\n",
    "source_path = pathlib.Path.cwd().resolve().parents[0] / 'data' / '1_wikibase_instance' / 'pike_cooper.csv'\n",
    "source_data = pandas.read_csv(source_path)\n",
    "items = list(source_data.item.unique())\n",
    "\n",
    "commencer = datetime.datetime.now()\n",
    "for n, x in enumerate(items):\n",
    "    time_to_finish = ((((datetime.datetime.now()-commencer)/(n+1))*(len(items)))+commencer).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'processed: {n+1} of {len(items)}; eta {time_to_finish}.')\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "    data = source_data.loc[source_data.item.isin([x])]\n",
    "    titles = list(data.title.unique())\n",
    "    year = list(data.year.unique())\n",
    "    pcid = list(data.item.unique()) \n",
    "    \n",
    "    label = {titles[0]}\n",
    "    director = [x for x in list(data.director.unique()) if x is not numpy.nan]\n",
    "\n",
    "    if len(director):\n",
    "        director = ' and '.join([x for x in director])\n",
    "        description = f'{year[0]} film by {director}' \n",
    "    else:\n",
    "        description = f'{year[0]} film' \n",
    "    \n",
    "    hook = add_item(titles[0], description, scaffold) \n",
    "    hook = pydash.get(hook, 'entity.id')\n",
    "    wb.claim.add(hook, prebuilt['instance of'], {\"entity-type\":\"item\",\"id\":prebuilt['cinematographic work']})\n",
    "    for t in titles:\n",
    "        wb.claim.add(hook, prebuilt['title'], t)   \n",
    "    wb.claim.add(hook, prebuilt['country of origin'], {\"entity-type\":\"item\",\"id\":prebuilt['Australia']})\n",
    "    wb.claim.add(hook, prebuilt['year'], {'time': f'+{year[0]}-00-00T00:00:00Z', 'timezone': 0, \n",
    "                                          'before': 0, 'after': 0, 'precision': 9, \n",
    "                                          'calendarmodel': 'http://www.wikidata.org/entity/Q1985727'})\n",
    "    wb.claim.add(hook, prebuilt['pike-cooper id'], str(pcid[0])) \n",
    "    \n",
    "print('all done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This code slightly clumsily gathers the data we have already committed,\n",
    "to allow access to the identifiers required for injecting the authority side.\n",
    "\"\"\"\n",
    "\n",
    "harvest_base_data = [wb.entity.get(pydash.get(x, 'entity.id')) for x in scaffold]\n",
    "\n",
    "build_film_data = pandas.DataFrame(columns=[\"item\", 'wikibase'])\n",
    "\n",
    "for x in harvest_base_data:\n",
    "    ident = list(pydash.get(x, 'entities').keys())[0]\n",
    "    pc = prebuilt['pike-cooper id']\n",
    "    search_location = f'entities.{ident}.claims.{pc}.0.mainsnak.datavalue.value'\n",
    "    pike_coop = pydash.get(x, search_location)\n",
    "    build_film_data.loc[len(build_film_data)] = [(pike_coop), (ident)]\n",
    "\n",
    "build_film_data = build_film_data.dropna()\n",
    "build_film_data['item'] = build_film_data['item'].astype('int64')\n",
    "\n",
    "build_film_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Director data is committed.\n",
    "\"\"\"\n",
    "\n",
    "source_path = pathlib.Path.cwd().resolve().parents[0] / 'data' / '1_wikibase_instance' / 'pike_cooper.csv'\n",
    "directors = pandas.read_csv(source_path)\n",
    "directors = pandas.merge(directors, build_film_data, on='item', how='left')\n",
    "directors = directors[['director', 'wikibase']].dropna()\n",
    "directors = directors.sort_values(by='director')\n",
    "unique_directors = list(directors.director.unique())\n",
    "credit_string = list()\n",
    "\n",
    "commencer = datetime.datetime.now()\n",
    "for n, x in enumerate(unique_directors):\n",
    "    time_to_finish = ((((datetime.datetime.now()-commencer)/(n+1))*(len(unique_directors)))+commencer).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'processed: {n+1} of {len(unique_directors)}; eta {time_to_finish}.')\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    data = directors.loc[directors.director.isin([x])]\n",
    "    hook = add_item(x, \"Australian Film Director\", scaffold)\n",
    "    hook = pydash.get(hook, 'entity.id')\n",
    "    for y in list(data.wikibase.unique()):\n",
    "        wb.claim.add(y, prebuilt['director'], {\"entity-type\":\"item\",\"id\":hook})   \n",
    "\n",
    "print('all done.')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This cell will generate a command which must be run from within the wikibase-docker location,\n",
    "and will produce a raw dump of all the data submitted so far for later matching purposes.\n",
    "\"\"\"\n",
    "\n",
    "result_address = str(pathlib.Path.cwd().resolve().parents[0] / 'data' / '1_wikibase_instance' / 'complete_export.json')\n",
    "print('docker-compose exec wikibase php ./extensions/Wikibase/repo/maintenance/dumpJson.php > '+result_address)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This code corrects a few elements of that export to make it a \"legal\" JSON.\n",
    "\"\"\"\n",
    "\n",
    "data_path = pathlib.Path.cwd().resolve().parents[0] / 'data' / '1_wikibase_instance' \n",
    "with open(data_path / 'complete_export_cleaned.json', 'w') as save_json:\n",
    "    with open(data_path / 'complete_export.json') as json_file:\n",
    "        json_file =  json_file.read()\n",
    "        json_file = json_file[(json_file.index('[')):]\n",
    "        for x in [f'Processed {y} entities.' for y in range(0, 9000)]:\n",
    "            json_file = json_file.replace(x,'')\n",
    "        save_json.write(json_file)  \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
