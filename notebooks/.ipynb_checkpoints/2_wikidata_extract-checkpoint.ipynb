{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WikiData extraction.**\n",
    "\n",
    "Blanket extraction of WikiData records for instances of Film (Q11424), or instances of subclasses of Film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1476 extracted.\n",
      "P495 extracted.\n",
      "P577 extracted.\n",
      "P345 extracted.\n",
      "P57 extracted.\n",
      "P162 extracted.\n",
      "P58 extracted.\n",
      "P344 extracted.\n",
      "P1040 extracted.\n",
      "P161 extracted.\n",
      "P725 extracted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from qwikidata.sparql import return_sparql_query_results\n",
    "import pandas, pathlib, pydash, numpy, string, unidecode\n",
    "\n",
    "def pull_clean(sparql):\n",
    "\n",
    "    \"\"\"\n",
    "    This function submits the SPARQL query, and cleans the resulting data.\n",
    "    \"\"\"\n",
    "\n",
    "    data = return_sparql_query_results(sparql)\n",
    "    dataframe = pandas.DataFrame.from_dict([x for x in pydash.get(data, 'results.bindings')]) \n",
    "    for k in list(dataframe.columns.values):\n",
    "        def extract_value(row, col):\n",
    "            return(pydash.get(row[col], 'value'))\n",
    "        dataframe[k] = dataframe.apply(extract_value, col=k, axis=1)\n",
    "    return(dataframe)\n",
    "\n",
    "\n",
    "def extract_property(wikidata_property):  \n",
    "    \n",
    "    \"\"\"\n",
    "    This function extracts a specific property for all relevant film pages.\n",
    "    The extraction is broken up into chunks based on \"publication date\" (P 577).\n",
    "    While it is probably possible to extract in a single pass with a powerful computer,\n",
    "    the intention is for this to be able to run on an entry level machine.\n",
    "    \"\"\"\n",
    "        \n",
    "    property_table = pandas.DataFrame()   \n",
    "\n",
    "    for year in [x for x in range(1880,2020)]:\n",
    "\n",
    "        query = \"\"\"SELECT DISTINCT ?item ?itemLabel ?value ?valueLabel \n",
    "             WHERE {?item p:P31/wdt:P279* ?item_s_0Statement .?item_s_0Statement ps:P31/wdt:P279* wd:Q11424.\n",
    "             ?item  wdt:P577 ?publication_date.\n",
    "             FILTER (YEAR(?publication_date) >= \"\"\"+str(year)+\"\"\")\n",
    "             FILTER (YEAR(?publication_date) < \"\"\"+str(year+1)+\"\"\")\n",
    "             OPTIONAL {?item wdt:\"\"\"+wikidata_property+\"\"\" ?value .}\n",
    "             SERVICE wikibase:label {bd:serviceParam wikibase:language \"en\". }}\"\"\"     \n",
    "        property_table = pandas.concat([property_table, pull_clean(query)])\n",
    "\n",
    "    query = \"\"\"SELECT DISTINCT ?item ?itemLabel ?value ?valueLabel \n",
    "        WHERE {?item p:P31/wdt:P279* ?item_s_0Statement .?item_s_0Statement ps:P31/wdt:P279* wd:Q11424.\n",
    "        FILTER NOT EXISTS { ?item wdt:P577 [] } \n",
    "        OPTIONAL {?item wdt:\"\"\"+wikidata_property+\"\"\" ?value .}\n",
    "        SERVICE wikibase:label {bd:serviceParam wikibase:language \"en\". }}\"\"\"\n",
    "    property_table = pandas.concat([property_table, pull_clean(query)]).drop_duplicates()\n",
    "\n",
    "    data_path = pathlib.Path.cwd().resolve().parents[0] / 'data' / '2_wikidata_extract' / 'props'\n",
    "    property_table.to_csv(data_path / f'{wikidata_property}.csv', index=False)\n",
    "    print(wikidata_property, 'extracted.')\n",
    "    \n",
    "wikidata_properties = ['P1476', 'P495', 'P577', 'P345', 'P57', 'P162', 'P58', 'P344', 'P1040', 'P161', 'P725']\n",
    "\n",
    "\"\"\"\n",
    "These properties are \"title\" (P1476), country of origin\" (P495), \"publication date\" (P577), \"IMDb ID\" (P345), \n",
    "\"director\" (P57), \"producer\" (P162), \"screenwriter\" (P58), \"director of photography\" (P344), \n",
    "\"film editor\" (P1040), \"cast member\" (P161) & \"voice actor\" (P725)\n",
    "\"\"\"                       \n",
    "                    \n",
    "for prop in wikidata_properties: \n",
    "    extract_property(prop)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliases extracted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Aside from the \"title\" property, WikiData also contains label 'aliases'.\n",
    "In the context of film records these are often alternate film titles.\n",
    "\n",
    "Note that the labels have already been collected, albeit only in English. \n",
    "This is fine for the Pike-Cooper dataset, but would need to be expanded for other uses.\n",
    "\"\"\"\n",
    "\n",
    "property_table = pandas.DataFrame()     \n",
    "for year in [x for x in range(1880,2020)]:\n",
    "\n",
    "    query = \"\"\"SELECT DISTINCT ?item ?alternative\n",
    "        WHERE {?item p:P31/wdt:P279* ?item_s_0Statement .?item_s_0Statement ps:P31/wdt:P279* wd:Q11424.\n",
    "        ?item  wdt:P577 ?publication_date.\n",
    "        FILTER (YEAR(?publication_date) >= \"\"\"+str(year)+\"\"\")\n",
    "        FILTER (YEAR(?publication_date) < \"\"\"+str(year+1)+\"\"\")\n",
    "        OPTIONAL { ?item skos:altLabel ?alternative . }}\"\"\"  \n",
    "    property_table = pandas.concat([property_table, pull_clean(query)])    \n",
    "\n",
    "query = \"\"\"SELECT DISTINCT ?item ?alternative\n",
    "        WHERE {?item p:P31/wdt:P279* ?item_s_0Statement .?item_s_0Statement ps:P31/wdt:P279* wd:Q11424.\n",
    "        ?item  wdt:P577 ?publication_date.\n",
    "        FILTER (YEAR(?publication_date) >= \"\"\"+str(year)+\"\"\")\n",
    "        FILTER (YEAR(?publication_date) < \"\"\"+str(year+1)+\"\"\")\n",
    "        OPTIONAL { ?item skos:altLabel ?alternative . }}\"\"\" \n",
    "property_table = pandas.concat([property_table, pull_clean(query)]).drop_duplicates()\n",
    "\n",
    "data_path = pathlib.Path.cwd().resolve().parents[0] / 'data' / '2_wikidata_extract' / 'props'\n",
    "property_table.to_csv(data_path / f'aliases.csv', index=False)\n",
    "print('aliases', 'extracted.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6932393\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>title</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>prop</th>\n",
       "      <th>label</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q59811424</td>\n",
       "      <td>Buffalo Running</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1883</td>\n",
       "      <td>P57</td>\n",
       "      <td>Eadweard Muybridge</td>\n",
       "      <td>Q190568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q59811424</td>\n",
       "      <td>Buffalo Running</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>1883</td>\n",
       "      <td>P57</td>\n",
       "      <td>Eadweard Muybridge</td>\n",
       "      <td>Q190568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q11766965</td>\n",
       "      <td>Man Walking Around a Corner</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1887</td>\n",
       "      <td>P57</td>\n",
       "      <td>Louis Le Prince</td>\n",
       "      <td>Q421675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q11766965</td>\n",
       "      <td>Man Walking Around a Corner</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1887</td>\n",
       "      <td>P344</td>\n",
       "      <td>Louis Le Prince</td>\n",
       "      <td>Q421675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q267176</td>\n",
       "      <td>Roundhay Garden Scene</td>\n",
       "      <td>England</td>\n",
       "      <td>1888</td>\n",
       "      <td>P57</td>\n",
       "      <td>Louis Le Prince</td>\n",
       "      <td>Q421675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        item                        title                   country  year  \\\n",
       "0  Q59811424              Buffalo Running            United Kingdom  1883   \n",
       "1  Q59811424              Buffalo Running  United States of America  1883   \n",
       "2  Q11766965  Man Walking Around a Corner            United Kingdom  1887   \n",
       "3  Q11766965  Man Walking Around a Corner            United Kingdom  1887   \n",
       "4    Q267176        Roundhay Garden Scene                   England  1888   \n",
       "\n",
       "   prop               label     link  \n",
       "0   P57  Eadweard Muybridge  Q190568  \n",
       "1   P57  Eadweard Muybridge  Q190568  \n",
       "2   P57     Louis Le Prince  Q421675  \n",
       "3  P344     Louis Le Prince  Q421675  \n",
       "4   P57     Louis Le Prince  Q421675  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "This code takes all of the disparate exports and combines into a single dataframe.\n",
    "It is \"exploded\" in the sense that the title information is duplicated for each credit.\n",
    "\"\"\"\n",
    "\n",
    "frame_path = pathlib.Path.cwd().resolve().parents[0] / 'data' / '2_wikidata_extract' \n",
    "data_path = pathlib.Path.cwd().resolve().parents[0] / 'data' / '2_wikidata_extract' / 'props'\n",
    "\n",
    "title_data = pandas.read_csv(data_path / 'P1476.csv')\n",
    "title_data = pandas.concat([title_data[['item', 'value']].rename(columns={'value':'title'}), \n",
    "                            title_data[['item', 'itemLabel']].rename(columns={'itemLabel':'title'})])\n",
    "\n",
    "aliases = pandas.read_csv(data_path / f'aliases.csv').rename(columns={'alternative':'title'})\n",
    "title_data = pandas.concat([title_data, aliases]).drop_duplicates()\n",
    "\n",
    "year_data = pandas.read_csv(data_path / 'P577.csv').rename(columns={'value':'year'})\n",
    "year_data['year'] = year_data['year'].str[:4]\n",
    "title_data = pandas.merge(title_data, year_data[['item', 'year']].drop_duplicates(), on='item', how='left')\n",
    "\n",
    "country_data = pandas.read_csv(data_path / 'P495.csv').rename(columns={'valueLabel':'country'})\n",
    "title_data = pandas.merge(title_data, country_data[['item', 'country']].drop_duplicates(), on='item', how='left')\n",
    "title_data['item'] = title_data['item'].str.split('/').str[-1]\n",
    "title_data = title_data.loc[title_data.item != title_data.title].drop_duplicates().dropna(subset=['title'])\n",
    "\n",
    "credit_data = pandas.DataFrame()\n",
    "for prop in ['P57', 'P58', 'P161', 'P344', 'P1040', 'P162', 'P725']:\n",
    "    dataframe = pandas.read_csv(data_path / f'{prop}.csv')\n",
    "    dataframe = dataframe[[x for x in list(dataframe.columns.values) if x != 'itemLabel']]\n",
    "    dataframe = dataframe.rename(columns={f'valueLabel':'label', 'value':'link'}).dropna()\n",
    "    dataframe['link'] = dataframe['link'].str.split('/').str[-1]\n",
    "    dataframe['prop'] = prop\n",
    "    credit_data = pandas.concat([credit_data, dataframe])\n",
    "\n",
    "credit_data['item'] = credit_data['item'].str.split('/').str[-1]    \n",
    "wikidata = pandas.merge(title_data, credit_data, on='item', how='left').drop_duplicates()\n",
    "\n",
    "frame_path = pathlib.Path.cwd().resolve().parents[0] / 'data' / '2_wikidata_extract' \n",
    "data = wikidata[['item', 'title', 'country', 'year', 'prop', 'label', 'link']].drop_duplicates() # drop title/label\n",
    "data.to_csv(frame_path / 'wikidata.csv', index=False)\n",
    "\n",
    "print(len(data))\n",
    "data.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
