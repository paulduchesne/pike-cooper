{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"pike_cooper_id\": 223,\n",
      "    \"year\": 1926,\n",
      "    \"title\": \"The Moth Of Moonbi\",\n",
      "    \"director\": \"Charles Chauvel\",\n",
      "    \"wikidata_id\": \"Q7752407\",\n",
      "    \"acmi_holdings\": [\n",
      "        \"16mm film; Access Print (Section 1)\"\n",
      "    ],\n",
      "    \"nfsa_holdings\": [\n",
      "        \"Film B&W Picture Dupe Negative B-Type\",\n",
      "        \"Film B&W Silent Release Print A-Type\",\n",
      "        \"Film B&W Silent Release Print B-Type\",\n",
      "        \"Tape (Moving Image) 1\\\" C Format\",\n",
      "        \"None Quicktime Movie [ARC]\",\n",
      "        \"Film B&W Silent Release Print Full Frame\",\n",
      "        \"Film B&W Picture Dupe Negative Full Frame\",\n",
      "        \"Film B&W Picture Dupe Negative - Interneg\",\n",
      "        \"Film B&W Silent Release Print\",\n",
      "        \"Disc (Moving Image) DVD : Digital Versatile Disk\",\n",
      "        \"Film B&W Picture Master Positive Full Frame\",\n",
      "        \"Disc (Moving Image) None\",\n",
      "        \"None Windows Media Video\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas\n",
    "import pathlib\n",
    "import pydash\n",
    "import requests\n",
    "\n",
    "def value_extract(row, column):\n",
    "\n",
    "    ''' Extract dictionary values. '''\n",
    "    \n",
    "    return pydash.get(row[column], 'value')\n",
    "\n",
    "def sparql_query(query, service):\n",
    "\n",
    "    ''' Send sparql request, and formulate results into a dataframe. '''\n",
    "\n",
    "    response = requests.get(service, params={'format': 'json', 'query': query}, timeout=120)\n",
    "    results = pydash.get(response.json(), 'results.bindings')\n",
    "    df = pandas.DataFrame.from_dict(results)\n",
    "    for column in df.columns:\n",
    "        df[column] = df.apply(value_extract, column=column, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def acmi_holdings(wikidata_id):\n",
    "\n",
    "    ''' Pull ACMI holdings from public API. '''\n",
    "\n",
    "    query = '''\n",
    "        select distinct ?wikidata_id ?acmi_id where {\n",
    "            values ?wikidata_id {'''+' '.join(['wd:'+x for x in wikidata_id])+'''}\n",
    "            ?wikidata_id wdt:P7003 ?acmi_id\n",
    "            } '''\n",
    "\n",
    "    acmi_tsv = pandas.read_csv('https://raw.githubusercontent.com/ACMILabs/acmi-api/main/app/tsv/works.tsv', delimiter='\\t')\n",
    "    acmi_tsv['acmi_id'] = 'works/'+acmi_tsv['id'].astype(str)\n",
    "    acmi_tsv = acmi_tsv.rename(columns={'holdings':'acmi_holdings'})\n",
    "    acmi_tsv['acmi_holdings'] = acmi_tsv['acmi_holdings'].str.split(',')\n",
    "    acmi_tsv = acmi_tsv.explode('acmi_holdings')\n",
    "    acmi_tsv = acmi_tsv[['acmi_id', 'acmi_holdings']].dropna()\n",
    "\n",
    "    df = sparql_query(query, 'https://query.wikidata.org/sparql').drop_duplicates()\n",
    "    df['wikidata_id'] = df['wikidata_id'].str.split('/').str[-1]\n",
    "    df = pandas.merge(df, acmi_tsv, on='acmi_id', how='left').dropna()\n",
    "    df = df[['wikidata_id', 'acmi_holdings']].drop_duplicates()\n",
    "    df = df.pivot_table(index=['wikidata_id'], aggfunc=lambda x: ','.join(sorted(x.unique()))).reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "def nfsa_holdings(wikidata_id):\n",
    "\n",
    "    ''' Pull NFSA holdings from public API. '''\n",
    "\n",
    "    query = '''\n",
    "        select distinct ?wikidata_id ?nfsa_id where {\n",
    "            ?wikidata_id wdt:P11948 ?nfsa_id\n",
    "            } '''\n",
    "\n",
    "    df = sparql_query(query, 'https://query.wikidata.org/sparql').drop_duplicates()\n",
    "    df['wikidata_id'] = df['wikidata_id'].str.split('/').str[-1]\n",
    "    df = df.loc[df.wikidata_id.isin(list(wikidata_id))]\n",
    "\n",
    "    nfsa_dataframe = pandas.DataFrame(columns=['nfsa_id', 'nfsa_holdings'])\n",
    "    for x in df.nfsa_id.unique():\n",
    "        r = requests.get(f'https://api.collection.nfsa.gov.au/title/{x}')\n",
    "        if r.status_code == 200:\n",
    "            holdings = [y for y in json.loads(r.text)['media']]\n",
    "            holdings = pydash.uniq([f\"{y['itemType']} {y['mediaFormatName']}\" for y in holdings])\n",
    "            nfsa_dataframe.loc[len(nfsa_dataframe)] = [(x), (','.join(holdings))]\n",
    "        else:\n",
    "            raise Exception('NFSA API failure.')\n",
    "\n",
    "    df = pandas.merge(df, nfsa_dataframe, on='nfsa_id', how='left')\n",
    "    df = df[['wikidata_id', 'nfsa_holdings']]\n",
    "\n",
    "    return df\n",
    "\n",
    "dataframe = pandas.read_csv(pathlib.Path.cwd() / 'pike_cooper.csv')\n",
    "dataframe = pandas.merge(dataframe, acmi_holdings(dataframe.wikidata_id.unique()), on='wikidata_id', how='left')\n",
    "dataframe = pandas.merge(dataframe, nfsa_holdings(dataframe.wikidata_id.unique()), on='wikidata_id', how='left')\n",
    "dataframe = dataframe.fillna('')\n",
    "\n",
    "for x in [x for x in dataframe.to_dict('records') if x['pike_cooper_id'] == 223]:\n",
    "    for y in ['acmi_holdings', 'nfsa_holdings']:\n",
    "        x[y] = x[y].split(',')\n",
    "\n",
    "    print(json.dumps(x, indent=4, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
